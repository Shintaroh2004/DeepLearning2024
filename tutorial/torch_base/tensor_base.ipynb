{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二回 tensor型ってなに？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回からPytorchの少し細かい部分に首を突っ込んでいきます。まず、最初は前回出てきた「Pytorch専用配列」こと、tensor型についてです。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "１、tensor型のコンセプト"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor型とは、Pytorchの計算で用いられる型のことであり、一見、int型やfloat型と全く同じような性質を持ってるように思えますが、実は違うものです。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このtensor型は、Pytorchで作ったNNのパラメータ(重みやバイアス)、画像データやテキストデータの表現に使われ、並列計算をより効率的に行えるよう制作されています。とりあえず例を見ていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "tensor(10)\n"
     ]
    }
   ],
   "source": [
    "import torch #tensor型は、torchライブラリを読み込まないと使えない\n",
    "\n",
    "data=10\n",
    "\n",
    "#tensor型の変数は、torchの中のtensorクラスのインスタンス\n",
    "tensor_data=torch.tensor(data)\n",
    "\n",
    "print(data)\n",
    "print(tensor_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、print()関数で表示したとき、tensor型のデータは通常の整数型とは異なり、「tensor()」という文字が出てきて表示されます。これは、「tensor_data」と呼ばれる変数が、tensor型であることを意味しています。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ただこれだけだと「え？ただの数値じゃね?」って思うかもしれないんですが、次の例を見ていただけると分かります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のコードは、あるサイズが同じ配列どうしで、添え字(index)が等しい要素同士の和を求め、同じサイズの配列として出力を行うというコードです。（配列同士の足し算ですね）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 8, 10, 12]\n"
     ]
    }
   ],
   "source": [
    "#なんか違和感のある代入の仕方ですが、これはアンパック代入と言います\n",
    "array_1,array_2=[1,2,3,4],[5,6,7,8]\n",
    "\n",
    "#足し合わせた結果を入れるための配列\n",
    "sum_array=[]\n",
    "\n",
    "for i in range(0,4):\n",
    "#appendは配列に新しい要素を足し合わせるためのメソッドです。\n",
    "    sum_array.append(array_1[i]+array_2[i])\n",
    "#ここでは、array_1,2のi番目の要素を足し合わせてします。\n",
    "\n",
    "print(sum_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常のpythonコードの場合、この配列同士の和であっても、for文を用いて求める必要があるので、正直どちゃくそめんどいです。しかし、tensor型は、この手間を解消してくれます。次のコードは、tensor型の配列同士の足し算をしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6,  8, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#これはアンパック代入と呼ばれる代入の仕方ですが、pythonでも普通にできます\n",
    "tensor_array1,tensor_array2=torch.tensor([1,2,3,4]),torch.tensor([5,6,7,8])\n",
    "\n",
    "#配列同士足しちゃってるぞ?! これはいったい、どうなっちゃうんだー!!?\n",
    "sum_tensor=tensor_array1+tensor_array2\n",
    "\n",
    "print(sum_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、tensor型の配列は、for文なんて使わなくても勝手に要素同士の和を求めてくれます。この機能のことを「ブロードキャスト」といいます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このブロードキャストは、pytorch以外にも、numpyというライブラリで利用することができますがこの両者には圧倒的な違いが存在します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それは、「この機能をGPUで使えるか、否か」です。pytorchのtensor型は、GPUでもこのブロードキャスト演算を行えますが、numpyは不可能なんです。深層学習ではこういったブロードキャストの処理を素早く行う必要があり、GPUの並列処理能力が不可欠なんです。だからこそ、pytorchではデータをtensor型にする必要があるのです。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、tensor型の演算"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、このブロードキャスト機能を用いたpytorchによる演算を見ていきましょう。\n",
    "あ、ついでなんですけど、次からする計算は2年生の「線形代数」っていう教科でよく出てくるのでもしかしたら授業の手助けになるかもです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#その１、ベクトルと、行列\n",
    "\n",
    "#tensor型のベクトルの生成\n",
    "vec1=torch.tensor([1,2,3,4])\n",
    "\n",
    "vec2=torch.tensor([4,5,6,7])\n",
    "\n",
    "#tesor型の行列の生成\n",
    "mat1=torch.tensor(\n",
    "    [[0,1,2],\n",
    "    [10,11,12]]\n",
    ")\n",
    "\n",
    "mat2=torch.tensor(\n",
    "    [[3,4,5],\n",
    "    [13,14,15]]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchでは、このように「tensor()」の中に配列（二次元配列）を挿入することで、ベクトル、行列を作ることができます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、ベクトルと行列って言葉が出てきました。1年生（次の2年生）の方向けに解説しておきます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベクトル...大きさだけでなく、向きを持った量のこと。ここでは、配列のように複数のデータを持ったデータ構造として捉える。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列...文字通り、数字や文字などを縦と横で並べたもの。2次元配列のような、ベクトルのベクトルというふうに捉えておこう。行列で表されたデータの代表としては「グレースケール画像」がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11, 12, 13, 14])\n",
      "tensor([-4, -3, -2, -1])\n",
      "tensor([ 4,  8, 12, 16])\n",
      "tensor([0.1000, 0.2000, 0.3000, 0.4000])\n",
      "tensor([[10, 11, 12],\n",
      "        [20, 21, 22]])\n",
      "tensor([[-5, -4, -3],\n",
      "        [ 5,  6,  7]])\n",
      "tensor([[ 0,  4,  8],\n",
      "        [40, 44, 48]])\n",
      "tensor([[0.0000, 0.1000, 0.2000],\n",
      "        [1.0000, 1.1000, 1.2000]])\n"
     ]
    }
   ],
   "source": [
    "#その２、ベクトル、行列とただの数値の四則演算\n",
    "\n",
    "#ベクトル\n",
    "print(vec1+10)\n",
    "print(vec1-5)\n",
    "print(vec1*4)\n",
    "print(vec1/10) #ここが大事\n",
    "\n",
    "#行列\n",
    "print(mat1+10)\n",
    "print(mat1-5)\n",
    "print(mat1*4)\n",
    "print(mat1/10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、ただの数値とベクトル、行列の四則演算では、全ての要素に対して、ただの数値が足されたり、かけられたりしていますね。これもブロードキャストの能力の一つです。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ついでに言うと、ベクトル、行列とは異なる「ただの数値」のことを、「スカラー量」と言います。補足ですが、線形代数ではスカラー量とベクトル・行列の和差なんて定義されていないのでPytorchオンリーの計算であると思って下さい。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さっきのコードで「ここが大事」って書いてあるところのポイントを解説します。\n",
    "まずは、一体何が大事なのかという点ですね。ここで大事なのは、tensor型であっても、数値の型の変換自体は自動で行われるということです。（？）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のコードで説明していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "#補足\n",
    "print(vec1.dtype) #このdtypeは、torch.tensor型の属性(クラス自身がもつ変数みたいなもの)の一つです。\n",
    "print((vec1/10).dtype)#このdtypeには、tensor型がもつデータの型情報が代入されています。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchのtensor型と言っても、その中でも複数個の型に分けられています。この場合ですと、整数はtorch.int64、小数を含むデータの場合はtorch.float32となっております。このようにして、元はintのtensor型であっても、演算によってfloatのデータに変えてくれるということなんですね。ブロードキャスト恐るべし。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  7,  9, 11])\n",
      "tensor([-3, -3, -3, -3])\n",
      "tensor([ 4, 10, 18, 28])\n",
      "tensor([0.2500, 0.4000, 0.5000, 0.5714])\n",
      "tensor([[ 6,  8, 10],\n",
      "        [16, 18, 20]])\n",
      "tensor([[-6, -6, -6],\n",
      "        [ 4,  4,  4]])\n",
      "tensor([[ 0,  7, 16],\n",
      "        [60, 77, 96]])\n",
      "tensor([[0.0000, 0.1429, 0.2500],\n",
      "        [1.6667, 1.5714, 1.5000]])\n",
      "tensor([[ 3,  5,  7],\n",
      "        [23, 25, 27]])\n",
      "tensor([[-3, -3, -3],\n",
      "        [-3, -3, -3]])\n",
      "tensor([[  0,   4,  10],\n",
      "        [130, 154, 180]])\n",
      "tensor([[0.0000, 0.2500, 0.4000],\n",
      "        [0.7692, 0.7857, 0.8000]])\n"
     ]
    }
   ],
   "source": [
    "#その３、ベクトル or 行列　同士の四則演算\n",
    "\n",
    "vec1=torch.tensor([1,2,3,4])\n",
    "vec2=torch.tensor([4,5,6,7])\n",
    "vec3=torch.tensor([6,7,8])\n",
    "\n",
    "mat1=torch.tensor(\n",
    "    [[0,1,2],\n",
    "    [10,11,12]]\n",
    ")\n",
    "\n",
    "mat2=torch.tensor(\n",
    "    [[3,4,5],\n",
    "    [13,14,15]]\n",
    ")\n",
    "\n",
    "#ベクトルとベクトル\n",
    "print(vec1+vec2)\n",
    "print(vec1-vec2)\n",
    "print(vec1*vec2)\n",
    "print(vec1/vec2)\n",
    "\n",
    "#行列とベクトル\n",
    "print(mat1+vec3)\n",
    "print(mat1-vec3)\n",
    "print(mat1*vec3)\n",
    "print(mat1/vec3)\n",
    "\n",
    "#行列と行列\n",
    "print(mat1+mat2)\n",
    "print(mat1-mat2)\n",
    "print(mat1*mat2)\n",
    "print(mat1/mat2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ブロードキャストの力によって、各要素同士での演算が可能になっています。行列とベクトルの演算は特別で、行列が含む各ベクトルと、ベクトル同士の演算で表されています。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし、2、3年生(次の3,4年生)にとったら、少し違和感のある箇所があります。それは、掛け算のところです。本来の行列・ベクトルの掛け算は、もっと異なる形で定義されているはずですが、Pytorchでは要素同士の掛け算で表されています。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに驚くべき点は割り算です。通常の行列同士の割り算なんて定義されていなかったはずです。そのため、一般的な数学と、Pytorchの行列・ベクトルの乗除についてはっきり区別しておく必要があります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようなPytorchにおける行列・ベクトル同士の乗除を「アダマール積」といいます。1年生(次の2年生)にとったら残念ですが、今回は一般数学における行列・ベクトルの乗除については触れることはありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 2, 1],\n",
      "        [6, 5, 4],\n",
      "        [9, 8, 7]])\n",
      "tensor([[3, 6, 9],\n",
      "        [2, 5, 8],\n",
      "        [1, 4, 7]])\n"
     ]
    }
   ],
   "source": [
    "#その４、転置\n",
    "\n",
    "mat3=torch.tensor(\n",
    "    [[3,2,1],\n",
    "    [6,5,4],\n",
    "    [9,8,7]]\n",
    ")\n",
    "\n",
    "print(mat3)\n",
    "print(mat3.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "転置とは、行列の行と列を逆にする事です。つまり、i行のj列にある要素を、j行のi列にもっていくという事です。分かりにくいと思うので、コードで示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 6, 9], [2, 5, 8], [1, 4, 7]]\n"
     ]
    }
   ],
   "source": [
    "mat4=[[3,2,1],\n",
    "    [6,5,4],\n",
    "    [9,8,7]]\n",
    "\n",
    "#いったん空の行列を作る\n",
    "mat5=[[0,0,0],\n",
    "    [0,0,0],\n",
    "    [0,0,0]]\n",
    "\n",
    "for i in range(0,3):\n",
    "    for j in range(0,3):\n",
    "        mat5[j][i]=mat4[i][j] #ここ見るとよくわかる\n",
    "\n",
    "print(mat5) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、torchのtensor生成と、加工"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorchには、tensor型のデータを生成する方法がいくつかあります。１つは、これまでのように、自分たちで定義した配列を代入してtensor型にする方法です。そして2つ目として、torchのモジュールを用いる方法があります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は、torchのモジュールによって、tensor型の行列・ベクトルを生成しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5322, 0.4569, 0.4682])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([[0.1922, 0.6777, 0.2448, 0.7856, 0.9380],\n",
      "        [0.4102, 0.4485, 0.5694, 0.5236, 0.1905],\n",
      "        [0.3857, 0.4275, 0.5215, 0.8969, 0.8357],\n",
      "        [0.8670, 0.1209, 0.2396, 0.5318, 0.2857]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#これが、tensor行列や、ベクトルのサイズをしめす引数となる\n",
    "shape1=(3)\n",
    "shape2=(4,5)\n",
    "\n",
    "#ベクトルの生成\n",
    "rand_vec=torch.rand(shape1) #ランダムな値\n",
    "one_vec=torch.ones(shape1) #全て1\n",
    "zero_vec=torch.zeros(shape1) #全て0\n",
    "\n",
    "print(rand_vec)\n",
    "print(one_vec)\n",
    "print(zero_vec)\n",
    "\n",
    "#行列の生成\n",
    "rand_mat=torch.rand(shape2)\n",
    "one_mat=torch.ones(shape2)\n",
    "zero_mat=torch.zeros(shape2)\n",
    "\n",
    "print(rand_mat)\n",
    "print(one_mat)\n",
    "print(zero_mat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにして、pytorchはある一定のパターンを持つ行列を簡単に生成することができます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、tensor型のベクトル・行列は、配列と同じように扱える場面があり、ある要素だけ値を変更するといったことも可能です。ここでは、Pythonのスライス機能等を用いて、値を変化させてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 0., 1., 1.])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 9., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "shape=(4,4)\n",
    "\n",
    "tensor=torch.ones(shape)\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "print(tensor[0]) #4*4の行列の1行目を表示する\n",
    "print(tensor[:,0]) #4*4行列の1列目を表示\n",
    "\n",
    "tensor[:,1]=0\n",
    "\n",
    "print(tensor[1])\n",
    "print(tensor[:,1])\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "tensor[:,:]=3\n",
    "print(tensor)\n",
    "\n",
    "tensor[2,2]=9\n",
    "print(tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、特定の列・行の値を変えることもできるし、何行何列の要素だけ変更するということもできます"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、ふと気になるのは[:,1]という表現ですね。この[:]ってどういう意味でしょう?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor=torch.tensor([1,2,3])\n",
    "print(tensor[:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この[:]は配列のある列や行の要素すべてのことを指します。上の例のように、tensorの値がすべて表示されていますね。これはよく出てくる表現なので覚えておきましょう。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、tensor同士の結合をしてみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 4., 1.],\n",
      "        [1., 1., 4., 1.],\n",
      "        [1., 1., 4., 1.]])\n",
      "tensor([[1., 1., 4., 1., 1., 1., 4., 1., 1., 1., 4., 1.],\n",
      "        [1., 1., 4., 1., 1., 1., 4., 1., 1., 1., 4., 1.],\n",
      "        [1., 1., 4., 1., 1., 1., 4., 1., 1., 1., 4., 1.]])\n",
      "tensor([[1., 1., 4., 1.],\n",
      "        [1., 1., 4., 1.],\n",
      "        [1., 1., 4., 1.],\n",
      "        [1., 1., 4., 1.],\n",
      "        [1., 1., 4., 1.],\n",
      "        [1., 1., 4., 1.],\n",
      "        [1., 1., 4., 1.],\n",
      "        [1., 1., 4., 1.],\n",
      "        [1., 1., 4., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor=torch.ones(3,4)\n",
    "\n",
    "tensor[:,2]=4\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "t2=torch.cat([tensor,tensor,tensor],dim=1)\n",
    "print(t2)\n",
    "\n",
    "t3=torch.cat([tensor,tensor,tensor],dim=0)\n",
    "print(t3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このtorch.catは、tensorを配列に入れることで、そのtensor達をつなぎ合わせた新しいtensorを作ることができます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、tensorへの変換"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchにおいて、データを学習させるには、すべてのデータをtensor型に変換する必要がありました。しかし、学習データと呼ばれるものは、numpyで生成されたデータや、jpegといった画像データなどで、どれも変換が必要なものばかりです。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpyの変換自体はとても簡単です。torchの機能として、「torch.from_numpy()」はその一つです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\venv\\torch\\lib\\site-packages (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#まずはnumpyのインストールから。入ってる人でもスキップはされない\n",
    "%pip install numpy\n",
    "\n",
    "import numpy as np #numpyは慣用的にnpと略されて使用されます。\n",
    "\n",
    "n=np.ones(5)\n",
    "print(n)\n",
    "print(type(n))\n",
    "\n",
    "t=torch.from_numpy(n)\n",
    "print(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、tensorからnumpyへの変換も可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "n2=t.numpy()\n",
    "print(n2)\n",
    "print(type(n2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この両方向への変換はよく使うので覚えておくといいでしょう。しかも楽ですもんね。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は画像です。こいつはtorchだけでは無理です。そこで、「pillow」と呼ばれる画像読み込みライブラリで読み込んだ画像をtensorに変換する「torchvision」を使います。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像データ自体は「images」フォルダの「to_tensor.jpg」使います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\user\\venv\\torch\\lib\\site-packages (9.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "#pillowをインストール\n",
    "%pip install Pillow\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "#画像を読み込み\n",
    "img=Image.open('images/to_tensor.jpg') #これは「相対パス」表記で示された画像を読み込んでいます。\n",
    "\n",
    "#画像を変換\n",
    "tensor_img=torchvision.transforms.functional.to_tensor(img)\n",
    "print(tensor_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一応これで変換はできていますが、ちょっとデータが多すぎて省略表示になっていますね。というのも1ピクセル単位で読み込んで変換してるので無理はないです。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そういえば、「相対パス」って習いましたか?　習っていないなら近いうちに説明を加えた演習を行いたいので、別途メールでもいいので知らせていただけると嬉しいです。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回の演習は以上です。少し長かったですがお疲れ様でした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\user\\venv\\torch\\lib\\site-packages (23.0)\n",
      "Collecting pip\n",
      "  Using cached pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0\n",
      "    Uninstalling pip-23.0:\n",
      "      Successfully uninstalled pip-23.0\n",
      "Successfully installed pip-23.0.1\n"
     ]
    }
   ],
   "source": [
    "#おまけ、pip の upgrade は定期的に\n",
    "!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "15ceeb7393f0aff2609f088f7fc6d439204a45655252d34f7ade3b35005ea835"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
