{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一回 Pytorch クイックスタート\n",
    "pytorchを用いた画像認識を体験し、深層学習の流れを学ぶ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、ライブラリのインポート：\n",
    "pythonでは以下のようなコードでライブラリをインポートして用いることができます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ライブラリ名'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mライブラリ名\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#このコードは実行できまへん\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ライブラリ名'"
     ]
    }
   ],
   "source": [
    "import ライブラリ名\n",
    "#このコードは実行できません"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このやり方に沿ってpytorchをimportしてみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリを読み込んでも、その中でクラスや名前空間で機能が分けられている場合が多いです。\n",
    "pytorchでは「torch」の中の「nn」、「DataLoader」などが頻繫に用いられているが呼び出すには\n",
    "次のようにコードを記述する必要があります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn\n",
    "torch.utils.data.DataLoader\n",
    "#このように「.」で区切って定義していく\n",
    "#これは実行してもエラーは出ませんが何も起きない"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし、いちいち「.」を使って定義するのは面倒ですよね?\n",
    "よってライブラリの中でも、頻繁に使う機能に関しては工夫して\n",
    "importする必要があります。以下がその例です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こうすると「.」を挟まず呼び出すことが可能です。\n",
    "途中「torchvision」というライブラリが出てきていますが、\n",
    "これはpytorchで画像を扱うときに物凄く有用なライブラリです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn\n",
    "DataLoader\n",
    "#このように「.」なしで呼び出してもエラーが出ない"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、データセットのロード：\n",
    "それではimportした機能(モジュール)を用いて、\n",
    "学習用のデータと、検証用のデータを読み込んでみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# データセットをダウンロードとデータセットの読込\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    #画像のデータをテンソル型とよぶものにする\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは「dataset」と呼ばれる機能(モジュール)の中の「FashionMNIST」クラスを読み込んでいます。\n",
    "「FashionMNIST」クラスは28×28のグレースケール画像が6万枚入った画像データセットです。\n",
    "クラスのインスタンスを作るだけで、勝手に画像をダウンロードし、使える状態にしてくれます。\n",
    "\n",
    "ここで、「データセット」という単語が出てきました。\n",
    "データセットとは深層学習用のデータと、その画像と紐づけられたラベル\n",
    "(画像の内容を示す番号のようなもの)の集合のことです。\n",
    "pytorchでは予め用意されているデータセットがいくつかあり、直ぐに使える状態になっています。\n",
    "\n",
    "また、pytorchの場合だと、画像データを配列に直してそのまま利用することはできないので、代わりにtensor(テンソル)型と呼ばれるものに変更します。\n",
    "これは「pytorch専用の配列みたいなもの」と思ってもらって大丈夫です。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、データローダーの制作：\n",
    "pytorchではデータセットの内容を読み込み、\n",
    "学習時・検証時に利用するためのデータローダーというものが必要になります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "#データローダーの制作\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "#X=画像 y=画像Xのラベル(画像の内容を示す番号のこと)\n",
    "for X, y in test_dataloader:\n",
    "    #N=画像の枚数 C=画像のチャンネル数 H=画像の高さ W=画像の幅\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    #y.shape=ラベルの数 y.dtype=ラベルデータのデータ型(intとかcharとか)\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchでは「DataLoader」クラスを用いてデータローダーを制作します。\n",
    "Dataloaderクラスは主にデータセットとバッチサイズというものを引数に持ちます。\n",
    "バッチサイズとは、データローダーが一回のロードで読み込む画像の枚数のことです。\n",
    "今回の場合、データローダーのバッチサイズが64なので、6万枚の画像から64枚の画像\n",
    "を読み込むようになっています。\n",
    "なぜ、6万枚一気に読み込まないのかは「ミニバッチ法」と呼ばれる学習方法に由来するためです。（詳しくはwebか渡辺に）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、ニューラルネットワーク(NN)の構築：\n",
    "いよいよNNの構築に入ります。NN(ニューラルネットワーク)とは，複数のノード（点）により構成される層と，それらをつなぐエッジ（線）によって構成されるデータ構造のことで，深層学習させたAIの本体とも言ってよいでしょう．下の画像に，NNの例を示しておきます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.eaglys.co.jp/media/4dy19LFuwYpzEjRaJl8yealuEc4sAmvbM8gBPTML.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "層には，データを入力するための入力層，隠れ層（中間層），推論結果を出力する出力層の3つの層があります．各ノードには，エッジを伝ってデータが入力され，そのデータを「活性化関数」と呼ばれる関数で処理し，関数の出力結果を次の層に伝搬させます．また，エッジには重みという値が存在し，データはエッジを通るたびに，重みの分だけ値が倍増します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また，重み以外にもバイアスと呼ばれる値があります．重みが掛け算によってエッジを通るデータを変化させるのに対して，バイアスは，足し算によってデータを変化させます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNの出力は，この重みやバイアスの値によって変化するため，これらの値を最適化することによって出力をコントロールする事ができます．この工程を学習といいます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchではNNのテンプレートである「nn.Module」というクラスを継承することで、NNを構築することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNeuralNetwork\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\u001b[38;5;66;03m#nn.Moduleを継承する\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\u001b[38;5;66;03m#継承先のフィールドを初期化\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):#nn.Moduleを継承する\n",
    "    def __init__(self):\n",
    "        super().__init__()#継承先のフィールドを初期化\n",
    "        self.flatten = nn.Flatten()#画像は2次元テンソルなので、28*28の要素を持つ一次元のテンソルに変換\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            #入力は28*28(画像のピクセル数)で出力は10\n",
    "            nn.Linear(28*28, 512), #層（線形層）\n",
    "            nn.ReLU(), #活性化関数\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "#構築したNNのインスタンスを定義して、実体を作る\n",
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Flatten()というモジュールが出てきました。これは入力のデータの次元を1次元テンソルに変換するモジュールです(引数によっては2次元もいける)。\n",
    "なぜ、画像を一次元にするかというと、下で定義したnn.Sequentialの入力が一次元であるためです。\n",
    "このnn.SequentialはNNの本体のことで、入力が28*28の一次元となっているため入力データを一次元にする必要があります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5、損失関数とオプティマイザ：\n",
    "続いて損失関数とオプティマイザを用意します。\n",
    "損失関数はNNの出力結果と実際の答えとの差を求めるものでしたね。pytorchでは標準搭載されている損失関数がいくつかあり、ここでは「CrossEntoropyLoss()」関数を用います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#nnクラスのモジュールです"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "オプティマイザは損失関数で求めた誤差を小さくするためにNNのパラメータ(重みとバイアス)\n",
    "を調整するものでしたね。こちらもpytorchで標準搭載されているものがあり「SDG」というものを使います"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "#torchの中のoptimクラスのSGDモジュールです"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このSGDには主に2つの引数があります。NNのパラメータ(重みとバイアス)と学習率です\n",
    "学習率とは、パラメータの調節の大きさを示すものであり、大きくすると学習が早くなる分精度が出なくなり、小さくするとその逆になります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6、学習と検証：\n",
    "いよいよ学習の準備ですここでは自前の学習用関数を用意します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    #データセットのサイズを読み込み\n",
    "    size = len(dataloader.dataset)\n",
    "    #NNを訓練モードに変更\n",
    "    model.train()\n",
    "    #データローダーから画像とラベルを読み込む\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        #NNの予測した値\n",
    "        pred = model(X)\n",
    "        #誤差(損失)を取得\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #逆誤差伝搬\n",
    "        loss.backward()\n",
    "        #オプティマイザがパラメータを調整\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つぎに、NNの認識精度を示す検証関数を作ります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    #NNを検証モードに変更\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            #誤差(損失)の合計を計算\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            #NNの予測した判定のうち、正解した回数の合計\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習開始です!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.161135  [    0/60000]\n",
      "loss: 2.144341  [ 6400/60000]\n",
      "loss: 2.098985  [12800/60000]\n",
      "loss: 2.119114  [19200/60000]\n",
      "loss: 2.030149  [25600/60000]\n",
      "loss: 1.990340  [32000/60000]\n",
      "loss: 2.011037  [38400/60000]\n",
      "loss: 1.934530  [44800/60000]\n",
      "loss: 1.954581  [51200/60000]\n",
      "loss: 1.868467  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.2%, Avg loss: 1.869736 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.902672  [    0/60000]\n",
      "loss: 1.861884  [ 6400/60000]\n",
      "loss: 1.760754  [12800/60000]\n",
      "loss: 1.808159  [19200/60000]\n",
      "loss: 1.663081  [25600/60000]\n",
      "loss: 1.636727  [32000/60000]\n",
      "loss: 1.657014  [38400/60000]\n",
      "loss: 1.568646  [44800/60000]\n",
      "loss: 1.601552  [51200/60000]\n",
      "loss: 1.496796  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.512913 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.574377  [    0/60000]\n",
      "loss: 1.537365  [ 6400/60000]\n",
      "loss: 1.402971  [12800/60000]\n",
      "loss: 1.476188  [19200/60000]\n",
      "loss: 1.341795  [25600/60000]\n",
      "loss: 1.348737  [32000/60000]\n",
      "loss: 1.353803  [38400/60000]\n",
      "loss: 1.291263  [44800/60000]\n",
      "loss: 1.326323  [51200/60000]\n",
      "loss: 1.234892  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.254804 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.324047  [    0/60000]\n",
      "loss: 1.308738  [ 6400/60000]\n",
      "loss: 1.152133  [12800/60000]\n",
      "loss: 1.259695  [19200/60000]\n",
      "loss: 1.127672  [25600/60000]\n",
      "loss: 1.152556  [32000/60000]\n",
      "loss: 1.164556  [38400/60000]\n",
      "loss: 1.113847  [44800/60000]\n",
      "loss: 1.153150  [51200/60000]\n",
      "loss: 1.078435  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.092537 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.156233  [    0/60000]\n",
      "loss: 1.163012  [ 6400/60000]\n",
      "loss: 0.985549  [12800/60000]\n",
      "loss: 1.124399  [19200/60000]\n",
      "loss: 0.992177  [25600/60000]\n",
      "loss: 1.018460  [32000/60000]\n",
      "loss: 1.047439  [38400/60000]\n",
      "loss: 1.000330  [44800/60000]\n",
      "loss: 1.038693  [51200/60000]\n",
      "loss: 0.980633  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.987182 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7、NNの保存と使用：\n",
    "では、学習させたNNを保存してみましょう。pytorchのNNは、「pth」ファイルで保存されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"model.pth\")\n",
    "print(\"Saved PyTorch Model to model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存したNNは、以下のようなコードで読み込めます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、読み込んだNNを利用してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ラベル番号と画像に写るものの名前を対応させた配列\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "#検証モード\n",
    "model.eval()\n",
    "#検証データセットの１枚目の画像とラベルを読み込み\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "\n",
    "#この「with torch.no_grad」がないと、pythonが落ちる\n",
    "with torch.no_grad():\n",
    "    #NNの予測結果を代入\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どうでしょうか?NNの回答と正解が一致していましたか?\n",
    "以上で第一回は終了となります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コード引用:https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "15ceeb7393f0aff2609f088f7fc6d439204a45655252d34f7ade3b35005ea835"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
