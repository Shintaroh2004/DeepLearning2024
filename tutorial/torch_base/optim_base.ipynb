{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第5回 モデルのトレーニング（最適化）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はモデルの学習について見ていきます．本当は一気にトレーニング関数の説明まで行きたいところですが，まずは学習の原理について触れていきます．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1，恐れるな自動微分"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まあ，嫌な節のタイトルですね．なんせいきなり「微分」ですもんね．それに自動って...　しかし，この自動微分こそがpytorchのモデル学習の根幹と言えます．まずは微分とは何かという事を簡単に理解していきましょう．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微分とは，ある関数の微小区間での変化の割合を求める計算のことです．変化の割合は次のような式で求める事ができます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "y=lambda x: 2*x**2 #y=2x^2　を示す　lambdaというのは無名関数を作るのに使えます．\n",
    "x1=1\n",
    "x2=2\n",
    "\n",
    "dif=(y(x2)-y(x1))/(x2-x1) #difに変化の割合を代入\n",
    "\n",
    "print(dif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは，y=2x^2という関数があったとき，x が x1 から x2 へ変化した時の y の変化量を示しています．この場合，変化の区間は[x1，x2]であり，その大きさは x2-x1 で示せます．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで区間とは x が変化する幅である事がわかりましたね？ではこの区間を「めちゃくちゃ小さく」してみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "y=lambda x: 2*x**2 #y=2x^2　を示す　lambdaというのは無名関数を作るのに使えます．\n",
    "x1=1\n",
    "x2=1+0.00000001 #x1からの差は0.00000001\n",
    "\n",
    "dif=(y(x2)-y(x1))/(x2-x1) #difに変化の割合を代入\n",
    "\n",
    "print(dif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この滅茶苦茶小さくした区間のことを微小区間といい，微小区間での変化の割合を微分係数をといいます．そして，この微分係数を求めることを微分（数値微分）といいます．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つまり，微分とは超ちっちゃい区間での関数の変化の割合を求めるということです．しかし，これがいったい何の役に立つのでしょうか？それは，変数をどの向き（+ or -）に動かしたら関数の値が小さくなるのかを求めるためです．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "値の変化を考える関数を「損失関数」として考えてみましょう．損失関数の値は常に小さくなる方がいいですよね？そのため，２億個以上存在するパラメータ（重みやバイアス）の値をそれぞれ増やすか減らすかによって損失関数の値が増えるか減るかを求める必要があります．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つまり，損失関数の値が小さくなる「方向」（どのパラメータの値を＋するか，-するか）を定める必要があるわけです．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この計算をpytorchでは自動で行ってくれます．変数の種類が2つ以上の微分を偏微分といい，pytorchでは2億以上のパラメータでの偏微分を行いますが... ここからは3年後期の内容なので今は蓋をしておきます．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2， 損失関数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いよいよトレーニング関数までの道のりを歩いていきます．その前に下準備をしておきましょう．次のコードを実行してください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは損失関数からです．損失関数は，モデルの出力と正解の値（ラベル値）との誤差を求める関数でしたね．この損失関数にはいろんな種類があります．それぞれに適した使い道が存在するので代表的なのを見ておきましょう．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考URL https://qiita.com/lucasta1/items/3e0e4306940fc35e0af1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropyLoss...多値分類（出力が3つ以上）でも2値分類でも使える優秀な損失関数です．迷ったらこれ使え！ってやつです．理由としては最終出力の関数\n",
    "「SoftMax関数」との相性がとても良いからです．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さすがに詳しい定義式とかは省きます．難しすぎるのでね．代わりに呼び出し方を記しておきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5565, 0.1124, 0.1494, 0.1145, 0.0672])\n",
      "tensor([0., 0., 1., 0., 0.])\n",
      "loss is tensor(1.6776)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13112\\3846959998.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output=softmax(input) #ソフトマックスにデータ入力\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "loss_fn=nn.CrossEntropyLoss() #損失関数呼び出し\n",
    "\n",
    "input = torch.randn(5) #5つランダムな値\n",
    "softmax=nn.Softmax() #ソフトマックス層\n",
    "relu=nn.ReLU() #ReLU層\n",
    "input=relu(input) #ReLU層にデータデータを入力\n",
    "output=softmax(input) #ソフトマックスにデータ入力\n",
    "\n",
    "target = torch.zeros(5) #ラベルデータ\n",
    "target[2]=1 #正解ラベル2\n",
    "\n",
    "loss = loss_fn(output, target) #損失を出す\n",
    "\n",
    "print(output) #入力を表示\n",
    "print(target) #出力を表示\n",
    "print(\"loss is\",loss) #損失の値を表示"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Margin Loss...複数クラス分類（多値分類）に使われる関数．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1のloss=0.0 , x2のloss=0.32499998807907104\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.MultiMarginLoss() #マルチマージンロスの呼び出し\n",
    "\n",
    "x1 = torch.tensor([[0.0, 0.0, 0.0, 1.0]])\n",
    "x2 = torch.tensor([[0.1, 0.2, 0.4, 0.8]])\n",
    "y = torch.tensor([3]) #3が正解ラベル\n",
    "\n",
    "loss = loss_fn(x1, y) #正解時の損失の表示\n",
    "loss2 = loss_fn(x2, y) #正解に近いときの損失\n",
    "print(f\"x1のloss={loss} , x2のloss={loss2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kullback Leibler divergence Loss...KLダイバージェンスと呼ばれるもの．距離関数と呼ばれるものの一つ．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このKLダイバージェンスは理解がとても難しいです．ですが，分かりやすかったサイトがあるので示しておきます．\n",
    "https://dx-consultant-fast-evolving.com/the-meaning-of-kl-divergence/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KLダイバージェンスは主に生成系のモデルというものに使われます．例えばオートエンコーダとかですね．また，皆さんが聞いたことがあるものとして「novelAI」がありますが，これはまた別のダイバージェンス関数が使われています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2387, 0.2010, 0.5603], grad_fn=<SoftmaxBackward0>) tensor([0.4609, 0.1143, 0.4249], grad_fn=<SoftmaxBackward0>)\n",
      "tensor(-0.4465, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13112\\994188236.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = nn.Softmax()(_input)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13112\\994188236.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  target = nn.Softmax()(_target)\n",
      "c:\\Users\\user\\venv\\torch\\lib\\site-packages\\torch\\nn\\functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.KLDivLoss() #KLダイバージェンス呼び出し\n",
    "\n",
    "#ランダムなデータをソフトマックスにかけて，0~1にする．\n",
    "_input = torch.randn(3, requires_grad=True) \n",
    "input = nn.Softmax()(_input)\n",
    "_target = torch.randn(3, requires_grad=True)\n",
    "target = nn.Softmax()(_target)\n",
    "\n",
    "loss = loss_fn(input, target) #損失の算出\n",
    "print(input, target)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いろんな損失関数がありますね．今回はこの中でもCrossEntropyLossを使います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3，オプティマイザ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にオプティマイザを用意します．オプティマイザには損失関数の出力をもとにパラメータを調整するという役割があります．こちらにも種類がありますが基本的には最新のものを使うのがベストでしょう．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考　https://rightcode.co.jp/blog/information-technology/torch-optim-optimizer-compare-and-verify-update-process-and-performance-of-optimization-methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最新のものとしてはAdamWやAdamがありますが．その中でも一番シンプルなSGDを今回は使います．"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "オプティマイザは主にモデルのパラメータと，学習率の二つを引数に取ります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習率は，パラメータの値をどれほど変更させるかを示す値です．これが大きいと学習が早く進みますが，精度が低くなります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.adamw.AdamW"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#一応AdamとAdamWの呼び出し方も書いときます\n",
    "torch.optim.Adam\n",
    "torch.optim.AdamW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4，最適化を始める"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それではモデルの学習，つまり最適化を始めていきましょう．まずは，トレーニング関数の定義です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    #enumerateは要素のindexと要素自体を渡す組み込み関数\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        #モデルの出力を代入\n",
    "        pred = model(X)\n",
    "\n",
    "        #ラベルと出力を損失関数に代入し，損失を求める\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        #勾配の計算を初期化\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #ここで，どの方向に各パラメータを変化させることで\n",
    "        #損失が小さくなるか求めている．\n",
    "        loss.backward()\n",
    "\n",
    "        #最適化を進める\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch%100==0:\n",
    "            print(f\"loss {loss:>7f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで，学習自体はできるようになりましたが，性能検証用の関数も作っておきましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset) #データの数\n",
    "    num_batches = len(dataloader) #バッチサイズ\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    #ここでは微分の計算をしないのでwith torch.no_grad()を用いる\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches #損失の平均\n",
    "    correct /= size #モデルの正確さの平均を示す．\n",
    "    print(f\"Test Loss: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss 2.313912\n",
      "loss 2.298081\n",
      "loss 2.282833\n",
      "loss 2.278455\n",
      "loss 2.255876\n",
      "loss 2.227625\n",
      "loss 2.240412\n",
      "loss 2.203619\n",
      "loss 2.211767\n",
      "loss 2.174260\n",
      "Test Loss: \n",
      " Accuracy: 33.2%, Avg loss: 2.172681 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss 2.179456\n",
      "loss 2.170921\n",
      "loss 2.125327\n",
      "loss 2.150841\n",
      "loss 2.089914\n",
      "loss 2.036754\n",
      "loss 2.068535\n",
      "loss 1.990797\n",
      "loss 2.002300\n",
      "loss 1.929031\n",
      "Test Loss: \n",
      " Accuracy: 53.8%, Avg loss: 1.928287 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss 1.955051\n",
      "loss 1.926798\n",
      "loss 1.821610\n",
      "loss 1.870794\n",
      "loss 1.749757\n",
      "loss 1.700849\n",
      "loss 1.727181\n",
      "loss 1.621316\n",
      "loss 1.643120\n",
      "loss 1.536563\n",
      "Test Loss: \n",
      " Accuracy: 60.4%, Avg loss: 1.552305 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss 1.617959\n",
      "loss 1.577403\n",
      "loss 1.430790\n",
      "loss 1.510266\n",
      "loss 1.381901\n",
      "loss 1.377535\n",
      "loss 1.395194\n",
      "loss 1.311879\n",
      "loss 1.346512\n",
      "loss 1.243982\n",
      "Test Loss: \n",
      " Accuracy: 62.7%, Avg loss: 1.270038 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss 1.352180\n",
      "loss 1.326064\n",
      "loss 1.161542\n",
      "loss 1.275140\n",
      "loss 1.144875\n",
      "loss 1.170554\n",
      "loss 1.194862\n",
      "loss 1.124304\n",
      "loss 1.165596\n",
      "loss 1.076077\n",
      "Test Loss: \n",
      " Accuracy: 64.1%, Avg loss: 1.099391 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss 1.178275\n",
      "loss 1.171133\n",
      "loss 0.987894\n",
      "loss 1.131997\n",
      "loss 1.000263\n",
      "loss 1.033572\n",
      "loss 1.073890\n",
      "loss 1.005907\n",
      "loss 1.049878\n",
      "loss 0.971773\n",
      "Test Loss: \n",
      " Accuracy: 65.6%, Avg loss: 0.990935 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss 1.058868\n",
      "loss 1.071830\n",
      "loss 0.869837\n",
      "loss 1.037542\n",
      "loss 0.910188\n",
      "loss 0.937447\n",
      "loss 0.996165\n",
      "loss 0.928178\n",
      "loss 0.970307\n",
      "loss 0.902504\n",
      "Test Loss: \n",
      " Accuracy: 66.8%, Avg loss: 0.917645 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss 0.971215\n",
      "loss 1.002944\n",
      "loss 0.785828\n",
      "loss 0.971024\n",
      "loss 0.850866\n",
      "loss 0.867188\n",
      "loss 0.942216\n",
      "loss 0.875105\n",
      "loss 0.912770\n",
      "loss 0.853386\n",
      "Test Loss: \n",
      " Accuracy: 67.8%, Avg loss: 0.865077 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss 0.903534\n",
      "loss 0.951322\n",
      "loss 0.723117\n",
      "loss 0.921136\n",
      "loss 0.809137\n",
      "loss 0.814098\n",
      "loss 0.901720\n",
      "loss 0.837547\n",
      "loss 0.869675\n",
      "loss 0.816151\n",
      "Test Loss: \n",
      " Accuracy: 68.8%, Avg loss: 0.825247 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss 0.849146\n",
      "loss 0.910013\n",
      "loss 0.674527\n",
      "loss 0.882190\n",
      "loss 0.777709\n",
      "loss 0.773469\n",
      "loss 0.868942\n",
      "loss 0.809621\n",
      "loss 0.835950\n",
      "loss 0.786400\n",
      "Test Loss: \n",
      " Accuracy: 70.1%, Avg loss: 0.793573 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 #学習回数は10回\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで最適化が完了です．では，早速試してみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's answer is : tensor([5])\n",
      "The answer is : tensor(5)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "idx=random.randint(0,64) #0~64のランダムな値\n",
    "\n",
    "model.eval() #モデルを検証モードで呼び出し．\n",
    "\n",
    "with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            pred = model(X[idx])\n",
    "            print(\"model\\'s answer is :\",pred.argmax(1)) #最大値をモデルの出したこたえとする\n",
    "            print(\"The answer is :\",y[idx]) #実際の答え\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正確性が60%後半なのであやしいですが，しっかり予測できていますね．今回で，pytorchの基本的な機能の説明は終了となります。次回からはいよいよ使用事例の多いモデルについて学習していきます．お疲れ様でした．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15ceeb7393f0aff2609f088f7fc6d439204a45655252d34f7ade3b35005ea835"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
